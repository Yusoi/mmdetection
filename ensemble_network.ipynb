{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd8276-9a15-493d-9702-193567232dc8",
   "metadata": {
    "id": "7abd8276-9a15-493d-9702-193567232dc8"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "N_CHANNELS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e7996-329c-4b1e-9096-66673a04f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch\n",
    "from network_definitions.u_net import UNet\n",
    "from network_definitions.u_net2 import UNet2\n",
    "from network_definitions.fcn import FCN32s as FCN\n",
    "from network_definitions.simple_network import SimpleNet\n",
    "from network_definitions.pyramid_network import PyramidNet\n",
    "from torchvision.models.segmentation import fcn_resnet101 as FCN_Res101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d6836-3303-4a58-909e-e8ebbf7c36b8",
   "metadata": {
    "id": "125d6836-3303-4a58-909e-e8ebbf7c36b8"
   },
   "source": [
    "# Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8e490-235c-4b8b-86e4-5032f56012c5",
   "metadata": {
    "id": "a6d8e490-235c-4b8b-86e4-5032f56012c5",
    "outputId": "5f5e2dec-91aa-46b3-d42b-e40967cada91"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "class EnsembleDataset(Dataset):\n",
    "    \"\"\"Ensemble dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, inc_img=False, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.inc_img = inc_img\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10533 #13167\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        inp = cv2.imread(self.root_dir+\"/img/\"+str(idx)+\".png\", cv2.IMREAD_UNCHANGED)\n",
    "        n1 = io.imread(self.root_dir+\"/net1/\"+str(idx)+\".png\", cv2.IMREAD_UNCHANGED)[:,:,np.newaxis]\n",
    "        n2 = io.imread(self.root_dir+\"/net2/\"+str(idx)+\".png\", cv2.IMREAD_UNCHANGED)[:,:,np.newaxis]\n",
    "        n3 = io.imread(self.root_dir+\"/net3/\"+str(idx)+\".png\", cv2.IMREAD_UNCHANGED)[:,:,np.newaxis]\n",
    "        n4 = io.imread(self.root_dir+\"/net4/\"+str(idx)+\".png\", cv2.IMREAD_UNCHANGED)[:,:,np.newaxis]\n",
    "        n5 = io.imread(self.root_dir+\"/net5/\"+str(idx)+\".png\", cv2.IMREAD_UNCHANGED)[:,:,np.newaxis]\n",
    "        res = np.dstack((inp,n1,n2,n3,n4,n5))/255\n",
    "        gt = io.imread(self.root_dir+\"/gt/\"+str(idx)+\".png\", cv2.IMREAD_UNCHANGED)[:,:,np.newaxis]/255\n",
    "        \n",
    "        sample = {'name': idx, 'inp': res, 'gt': gt}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "from skimage.transform import resize\n",
    "from torchvision import transforms, utils\n",
    "    \n",
    "class Resize(object):\n",
    "    def __init__(self, size, n_channels):\n",
    "        self.size = size\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "    def __call__(self,sample):\n",
    "        name,inp,gt = sample[\"name\"],sample[\"inp\"],sample[\"gt\"]\n",
    "        \n",
    "        return {\"name\": name, \"inp\": resize(inp,(self.size,self.size,self.n_channels),preserve_range=True), \"gt\": resize(gt,(self.size,self.size,1),preserve_range=True)}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        name,inp,gt = sample[\"name\"],sample[\"inp\"],sample[\"gt\"]\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        inp = inp.transpose((2, 0, 1))\n",
    "        gt = gt.transpose((2, 0, 1))\n",
    "        return {\"name\": name, \n",
    "                \"inp\": torch.from_numpy(inp),\n",
    "                \"gt\": torch.from_numpy(gt)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8933d255-781d-47e7-a4df-af65c74a37a1",
   "metadata": {
    "id": "8933d255-781d-47e7-a4df-af65c74a37a1",
    "outputId": "b0e15282-db09-460f-e923-ed51732716d4"
   },
   "outputs": [],
   "source": [
    "trainset = EnsembleDataset(root_dir='data/coco_bitwise_or_reduced_ensemble_results', \n",
    "                           inc_img=True,\n",
    "                           transform=transforms.Compose([Resize(512,N_CHANNELS),\n",
    "                                                         ToTensor()]))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd411d2a-1831-42ce-9a2c-b8e61ef675ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79005d77-36ac-4f74-8b20-74f6c6ac50c5",
   "metadata": {
    "id": "79005d77-36ac-4f74-8b20-74f6c6ac50c5"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1957d-e096-404f-ba42-6e3f3cf717a5",
   "metadata": {
    "id": "04b1957d-e096-404f-ba42-6e3f3cf717a5"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#PATH = \"work_dirs/simplenet_1/\"\n",
    "\n",
    "\n",
    "def train(net, trainloader, criterion, optimizer, save_path, tensorboard_path, checkpoint=None):\n",
    "    \n",
    "    EPOCH = 0\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=tensorboard_path)\n",
    "    \n",
    "    if checkpoint != None:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        EPOCH = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        net.train()\n",
    "    \n",
    "    for epoch in range(EPOCH,100):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            im_seg = data[\"inp\"].to(device, dtype=torch.float)\n",
    "            im_res = data[\"gt\"].to(device, dtype=torch.float)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            output = net(im_seg.float())\n",
    "            loss = criterion(output.float(), im_res.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                \"\"\"print('[%d, %5d] segm loss: %.6f  class loss: %.6f  loss: %.6f' %\n",
    "                      (epoch + 1, i + 1, running_loss_segm / 50, running_loss_class / 50, running_loss / 50))\"\"\"\n",
    "                print('[%d, %5d] loss: %.6f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 1999))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "                input_ = im_seg.cpu().detach()\n",
    "                output_ = output.cpu().detach()\n",
    "                gt_output_ = im_res.cpu().detach()\n",
    "                \n",
    "                #output_ = torch.argmax(output_,1)\n",
    "                #print(output_.shape)\n",
    "                \n",
    "                input_ = input_.numpy()[0].transpose((1,2,0))\n",
    "                output_ = output_.numpy()[0].transpose((1,2,0))\n",
    "                \n",
    "                gt_output_ = gt_output_.numpy()[0].transpose((1,2,0)).squeeze(axis=2)\n",
    "                \n",
    "                fig, ax = plt.subplots(nrows=1, ncols=9, figsize=(15,15))\n",
    "                ax=ax.flat\n",
    "                \n",
    "                ax[0].set_title(\"Original Image\")\n",
    "                ax[0].imshow(input_[:,:,0:3])\n",
    "                \n",
    "                \n",
    "                for i in range(0,5):\n",
    "                    #ax.append(fig.add_subplot(2, 4, i+1))\n",
    "                    ax[i+1].set_title(\"Input \"+str(i+1))  # set title\n",
    "                    ax[i+1].imshow(input_[:,:,i+3],cmap='gray',vmin=0,vmax=1)\n",
    "                    \n",
    "                ax[6].set_title(\"Output\")  # set title\n",
    "                ax[6].imshow(output_,cmap='gray',vmin=0,vmax=1)\n",
    "                \n",
    "                ax[7].set_title(\"Output Rounded\")  # set title\n",
    "                ax[7].imshow(np.around(output_),cmap='gray',vmin=0,vmax=1)\n",
    "                \n",
    "                #ax.append(fig.add_subplot(2, 4, 7))\n",
    "                ax[8].set_title(\"Ground Truth\")  # set title\n",
    "                ax[8].imshow(gt_output_,cmap='gray',vmin=0,vmax=1)\n",
    "                \n",
    "                fig.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "        writer.add_scalar('Loss', loss, epoch)\n",
    "\n",
    "        if epoch % 2 == 1:        \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                }, save_path+\"epoch_\"+str(epoch+1)+\".pt\")\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ba74b-ede9-478e-9671-e4cc02c9c93b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "OPTIMIZER = \"SGD\"\n",
    "ACTIVATION = \"lrelu\"\n",
    "LOSS = \"BCELoss\"\n",
    "\n",
    "layers = [\"\"]\n",
    "\n",
    "#for layers in #[[(3,8,16),(3,16,32),(5,32,64),(5,64,32),(3,32,16),(3,16,2)]]:\n",
    "print(\"Starting training on network \",layers)\n",
    "    \n",
    "net = UNet2(N_CHANNELS,1)#layers,activation=ACTIVATION)\n",
    "net = net.to(device).float()\n",
    "\n",
    "if LOSS == \"BCELoss\":\n",
    "    criterion = nn.BCELoss()\n",
    "elif LOSS == \"CrossEntropyLoss\":\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if OPTIMIZER == \"SGD\":\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "elif OPTIMIZER == \"Adam\":\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "checkpoint_path = \"work_dirs/unet2_bitwise_or_img_ensemble_reduced_do_40\"\n",
    "for layer in layers:\n",
    "    checkpoint_path += \"_\"+str(layer)\n",
    "checkpoint_path += \"/\" + OPTIMIZER + \"_\" + ACTIVATION + \"_\" + LOSS + \"/\"\n",
    "tensorboard_path = checkpoint_path+\"tb/\"\n",
    "os.makedirs(tensorboard_path,exist_ok=True)\n",
    "\n",
    "train(net,trainloader,criterion,optimizer, checkpoint_path, tensorboard_path)#, checkpoint=\"work_dirs/simplenet_1/epoch_25.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d171ccb-b0e5-4dcc-9400-d8da4ba8e7d3",
   "metadata": {
    "id": "9d171ccb-b0e5-4dcc-9400-d8da4ba8e7d3",
    "outputId": "47ed91a7-6a3c-474d-e1b1-db1e2c43c4d7"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleNet([(3,8,16),(3,16,32),(3,32,64),(3,64,32),(3,32,16),(3,16,2)],activation=\"lrelu\").float().to(device)\n",
    "\n",
    "summary(model, (1,8,572,572))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d6c84-282f-4049-8d4e-2c30425b19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "event_acc = EventAccumulator('work_dirs/simplenet_1_1_1/sigmoid_BCELoss/tb')\n",
    "event_acc.Reload()\n",
    "# Show all tags in the log file\n",
    "print(event_acc.Tags())\n",
    "\n",
    "# E. g. get wall clock, number of steps and value for a scalar 'Accuracy'\n",
    "w_times, step_nums, vals = zip(*event_acc.Scalars('Loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04982b-5f79-4198-9209-67c014c52014",
   "metadata": {},
   "source": [
    "# Network Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87a84b-7af1-4c09-8fef-de23d2071730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1):\n",
    "    data = trainset[i]\n",
    "    \n",
    "    im_seg = data['im_seg']\n",
    "    im_res = data['im_res']\n",
    "    \n",
    "    res = im_seg[0:3,:,:].numpy().transpose((1,2,0))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.imshow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803644eb-b8fd-4f44-9954-658cea496322",
   "metadata": {
    "id": "803644eb-b8fd-4f44-9954-658cea496322"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57c266-e2bb-42cc-8dd0-a12cba789a7d",
   "metadata": {
    "id": "6e57c266-e2bb-42cc-8dd0-a12cba789a7d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33d1b8a1-a9f3-44b4-8834-bb5cabb22d55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb6ba8-4567-4899-9a91-640cbd8f6918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ensemble_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
