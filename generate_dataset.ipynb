{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26fb0fb4-4f59-480b-aacd-3811ac29805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average / weighted_average / bitwise_or\n",
    "ENSEMBLE_METHOD = \"bitwise_or\"\n",
    "VIABLE_COUNTABILITY = 0\n",
    "AVERAGE_ACCEPTABILITY = 0\n",
    "# Minimum value of credibility per mask\n",
    "CREDIBILITY_THRESHOLD = 0.6\n",
    "# Minimum IoU in order to group instances together\n",
    "DEVIATION_THRESHOLD = 0.5\n",
    "# Minimum IoU in order to compare instances while evaluating \n",
    "DEVIATION_THRESHOLD_EVAL = 0.5\n",
    "#For Weighted Average\n",
    "AVERAGE_ACCEPTABILITY_2 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd30065b-3897-4937-a569-dba032df03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_dict = {}\n",
    "\n",
    "#Mask R-CNN\n",
    "model_dict['mask_rcnn_X-101-64x4d-FPN'] = (('configs/mask_rcnn/mask_rcnn_x101_64x4d_fpn_mstrain-poly_3x_coco.py',\n",
    "                                            'checkpoints/mask_rcnn_x101_64x4d_fpn_mstrain-poly_3x_coco_20210526_120447-c376f129.pth',\n",
    "                                            'https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_x101_64x4d_fpn_mstrain-poly_3x_coco/mask_rcnn_x101_64x4d_fpn_mstrain-poly_3x_coco_20210526_120447-c376f129.pth'))\n",
    "\n",
    "#Cascade Mask R-CNN\n",
    "model_dict['cascade_mask_rcnn_X-101-64x4d-FPN'] = (('configs/cascade_rcnn/cascade_mask_rcnn_x101_64x4d_fpn_mstrain_3x_coco.py',\n",
    "                                                    'checkpoints/cascade_mask_rcnn_x101_64x4d_fpn_mstrain_3x_coco_20210719_210311-d3e64ba0.pth',\n",
    "                                                    'https://download.openmmlab.com/mmdetection/v2.0/cascade_rcnn/cascade_mask_rcnn_x101_64x4d_fpn_mstrain_3x_coco/cascade_mask_rcnn_x101_64x4d_fpn_mstrain_3x_coco_20210719_210311-d3e64ba0.pth'))\n",
    "#HTC\n",
    "model_dict['hybrid_task_cascade_mask_rcnn_X-101-64x4d-FPN'] = (('configs/htc/htc_x101_64x4d_fpn_dconv_c3-c5_mstrain_400_1400_16x1_20e_coco.py',\n",
    "                                                                'checkpoints/htc_x101_64x4d_fpn_dconv_c3-c5_mstrain_400_1400_16x1_20e_coco_20200312-946fd751.pth',\n",
    "                                                                'https://download.openmmlab.com/mmdetection/v2.0/htc/htc_x101_64x4d_fpn_dconv_c3-c5_mstrain_400_1400_16x1_20e_coco/htc_x101_64x4d_fpn_dconv_c3-c5_mstrain_400_1400_16x1_20e_coco_20200312-946fd751.pth'))\n",
    "#GCNet\n",
    "model_dict['gcnet_X-101-FPN_DCN_Cascade_Mask_GC(c3-c5,r4)'] = (('configs/gcnet/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_dconv_c3-c5_r4_gcb_c3-c5_1x_coco.py',\n",
    "                                                                'checkpoints/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_dconv_c3-c5_r4_gcb_c3-c5_1x_coco_20210615_161851-720338ec.pth',\n",
    "                                                                'https://download.openmmlab.com/mmdetection/v2.0/gcnet/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_dconv_c3-c5_r4_gcb_c3-c5_1x_coco/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_dconv_c3-c5_r4_gcb_c3-c5_1x_coco_20210615_161851-720338ec.pth'))\n",
    "#SCNet\n",
    "model_dict['scnet_X-101-64x4d-FPN'] = (('configs/scnet/scnet_x101_64x4d_fpn_20e_coco.py',\n",
    "                                        'checkpoints/scnet_x101_64x4d_fpn_20e_coco-fb09dec9.pth',\n",
    "                                        'https://download.openmmlab.com/mmdetection/v2.0/scnet/scnet_x101_64x4d_fpn_20e_coco/scnet_x101_64x4d_fpn_20e_coco-fb09dec9.pth'))\n",
    "\"\"\"\n",
    "#Carafe\n",
    "model_dict['mask_rcnn_r50_fpn_carafe_1x_coco'] = (('configs/carafe/mask_rcnn_r50_fpn_carafe_1x_coco.py',\n",
    "                                                   'checkpoints/mask_rcnn_r50_fpn_carafe_1x_coco_bbox_mAP-0.393__segm_mAP-0.358_20200503_135957-8687f195.pth',\n",
    "                                                   'https://download.openmmlab.com/mmdetection/v2.0/carafe/mask_rcnn_r50_fpn_carafe_1x_coco/mask_rcnn_r50_fpn_carafe_1x_coco_bbox_mAP-0.393__segm_mAP-0.358_20200503_135957-8687f195.pth'))\n",
    "#Deformable Convolution\n",
    "model_dict['cascade_mask_rcnn_x101_32x4d_fpn_dconv_c3-c5_1x_coco'] = (('configs/dcn/cascade_mask_rcnn_x101_32x4d_fpn_dconv_c3-c5_1x_coco.py',\n",
    "                                                                       'checkpoints/cascade_mask_rcnn_x101_32x4d_fpn_dconv_c3-c5_1x_coco-e75f90c8.pth',\n",
    "                                                                       'https://download.openmmlab.com/mmdetection/v2.0/dcn/cascade_mask_rcnn_x101_32x4d_fpn_dconv_c3-c5_1x_coco/cascade_mask_rcnn_x101_32x4d_fpn_dconv_c3-c5_1x_coco-e75f90c8.pth'))\n",
    "#Group Normalization\n",
    "model_dict['mask_rcnn_r101_fpn_gn-all_3x_coco'] = (('configs/gn/mask_rcnn_r101_fpn_gn-all_3x_coco.py',\n",
    "                                                    'checkpoints/mask_rcnn_r101_fpn_gn-all_3x_coco_20200513_181609-0df864f4.pth',\n",
    "                                                    'https://download.openmmlab.com/mmdetection/v2.0/gn/mask_rcnn_r101_fpn_gn-all_3x_coco/mask_rcnn_r101_fpn_gn-all_3x_coco_20200513_181609-0df864f4.pth'))\n",
    "#Group Normalization + Weight Standardization\n",
    "model_dict['mask_rcnn_r101_fpn_gn_ws-all_20_23_24e_coco'] = (('configs/gn+ws/mask_rcnn_r101_fpn_gn_ws-all_20_23_24e_coco.py',\n",
    "                                                              'checkpoints/mask_rcnn_r101_fpn_gn_ws-all_20_23_24e_coco_20200213-57b5a50f.pth',\n",
    "                                                              'https://download.openmmlab.com/mmdetection/v2.0/gn%2Bws/mask_rcnn_r101_fpn_gn_ws-all_20_23_24e_coco/mask_rcnn_r101_fpn_gn_ws-all_20_23_24e_coco_20200213-57b5a50f.pth'))\n",
    "#Detectors\n",
    "model_dict['detectors_htc_r101_20e_coco'] = (('configs/detectors/detectors_htc_r101_20e_coco.py',\n",
    "                   'checkpoints/detectors_htc_r101_20e_coco_20210419_203638-348d533b.pth',\n",
    "                   'https://download.openmmlab.com/mmdetection/v2.0/detectors/detectors_htc_r101_20e_coco/detectors_htc_r101_20e_coco_20210419_203638-348d533b.pth'))\"\"\"\n",
    "model_dict = list(model_dict.items())\n",
    "\n",
    "test_config = 'configs/common/mstrain-poly_3x_coco_instance.py'\n",
    "#test_config = 'configs/_base_/datasets/cityscapes_instance.py'\n",
    "dataset_name = os.path.splitext(test_config)[0].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f59362-f412-4c4e-a32b-7fe9411a2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def IoU(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "def IoU_Mask(maskA, maskB):\n",
    "    intersection = np.logical_and(maskA, maskB).astype(np.uint8)\n",
    "    union = np.logical_or(maskA, maskB).astype(np.uint8)\n",
    "    iou = np.sum(intersection)/np.sum(union)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f2d1fc-2c72-440b-a394-2ca4615d589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "from mmdet.datasets import build_dataset, build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmdet.apis import single_gpu_test\n",
    "from mmdet.models import build_detector\n",
    "from mmcv.runner import load_checkpoint\n",
    "from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n",
    "from typing import List, Tuple, Union, Dict\n",
    "from torch import nn\n",
    "from os import path\n",
    "from urllib import request\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import mmcv\n",
    "import os.path as osp\n",
    "import pycocotools.mask as mask_util\n",
    "\n",
    "COCO_CLASSES = ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "                'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n",
    "                'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
    "                'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',\n",
    "                'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "                'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n",
    "                'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "                'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "                'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\n",
    "                'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "                'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',\n",
    "                'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "                'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\n",
    "                'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush')\n",
    "\n",
    "CITYSCAPES_CLASSES = ('person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle',\n",
    "                      'bicycle')\n",
    "\n",
    "WORK_DIR = \"work_dirs/ensemble_results/\"\n",
    "\n",
    "def inference_on_dataset(model_info):\n",
    "    config, checkpoint = model_info\n",
    "    \n",
    "    cfg = Config.fromfile(config)\n",
    "\n",
    "    if cfg.get('custom_imports', None):\n",
    "        from mmcv.utils import import_modules_from_strings\n",
    "        import_modules_from_strings(**cfg['custom_imports'])\n",
    "    # set cudnn_benchmark\n",
    "    if cfg.get('cudnn_benchmark', False):\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    cfg.model.pretrained = None\n",
    "    if cfg.model.get('neck'):\n",
    "        if isinstance(cfg.model.neck, list):\n",
    "            for neck_cfg in cfg.model.neck:\n",
    "                if neck_cfg.get('rfp_backbone'):\n",
    "                    if neck_cfg.rfp_backbone.get('pretrained'):\n",
    "                        neck_cfg.rfp_backbone.pretrained = None\n",
    "        elif cfg.model.neck.get('rfp_backbone'):\n",
    "            if cfg.model.neck.rfp_backbone.get('pretrained'):\n",
    "                cfg.model.neck.rfp_backbone.pretrained = None\n",
    "                \n",
    "    test_cfg = Config.fromfile(test_config)\n",
    "    # in case the test dataset is concatenated\n",
    "    samples_per_gpu = 1\n",
    "    if isinstance(cfg.data.test, dict):\n",
    "        test_cfg.data.test.test_mode = True\n",
    "        samples_per_gpu = test_cfg.data.test.pop('samples_per_gpu', 1)\n",
    "        if samples_per_gpu > 1:\n",
    "            # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "            test_cfg.data.test.pipeline = replace_ImageToTensor(\n",
    "                test_cfg.data.test.pipeline)\n",
    "    elif isinstance(test_cfg.data.test, list):\n",
    "        for ds_cfg in test_cfg.data.test:\n",
    "            ds_cfg.test_mode = True\n",
    "        samples_per_gpu = max(\n",
    "            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in test_cfg.data.test])\n",
    "        if samples_per_gpu > 1:\n",
    "            for ds_cfg in test_cfg.data.test:\n",
    "                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "    # init distributed env first, since logger depends on the dist info.\n",
    "    distributed = False\n",
    "\n",
    "    #rank, _ = get_dist_info()\n",
    "    # allows not to create\n",
    "    #mmcv.mkdir_or_exist(osp.abspath(args.work_dir))\n",
    "    #timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "    #json_file = osp.join(args.work_dir, f'eval_{timestamp}.json')\n",
    "\n",
    "    # build the dataloader\n",
    "    dataset = build_dataset(test_cfg.data.test)\n",
    "    data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        samples_per_gpu=samples_per_gpu,\n",
    "        workers_per_gpu=test_cfg.data.workers_per_gpu,\n",
    "        dist=distributed,\n",
    "        shuffle=False)\n",
    "\n",
    "    # build the model and load checkpoint\n",
    "    cfg.model.train_cfg = None\n",
    "    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "    fp16_cfg = cfg.get('fp16', None)\n",
    "    if fp16_cfg is not None:\n",
    "        wrap_fp16_model(model)\n",
    "    checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n",
    "    #if args.fuse_conv_bn:\n",
    "        #model = fuse_conv_bn(model)\n",
    "    # old versions did not save class info in checkpoints, this walkaround is\n",
    "    # for backward compatibility\n",
    "    if 'CLASSES' in checkpoint.get('meta', {}):\n",
    "        model.CLASSES = checkpoint['meta']['CLASSES']\n",
    "    else:\n",
    "        model.CLASSES = dataset.CLASSES\n",
    "\n",
    "    #classes = model.CLASSES.copy()\n",
    "    classes = list(enumerate(model.CLASSES)).copy()\n",
    "    model = MMDataParallel(model, device_ids=[0])\n",
    "    outputs = single_gpu_test(model, data_loader, None, None, None)\n",
    "    \n",
    "    print(len(dataset))\n",
    "    \n",
    "    return outputs, classes, len(dataset)\n",
    "\n",
    "\n",
    "def gather_results_from_model(model_name: str, model_info: Tuple[str,str], score_thr:float, person_only:bool , result_type = 'bbox'):\n",
    "    if not osp.exists(WORK_DIR+model_name+\"_\"+dataset_name+\".pkl\"):\n",
    "        mmcv.mkdir_or_exist(osp.abspath(WORK_DIR))\n",
    "        results, original_classes, dataset_size = inference_on_dataset(model_info)\n",
    "        classes = original_classes\n",
    "        mmcv.dump(results, WORK_DIR+model_name+\"_\"+dataset_name+\".pkl\")\n",
    "    else:\n",
    "        config,checkpoint = model_info\n",
    "        cfg = Config.fromfile(config)\n",
    "        model = build_detector(cfg.model)\n",
    "        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n",
    "        if 'CLASSES' in checkpoint.get('meta', {}):\n",
    "            model.CLASSES = checkpoint['meta']['CLASSES']\n",
    "        else:\n",
    "            model.CLASSES = dataset.CLASSES\n",
    "        original_classes = list(enumerate(model.CLASSES)).copy()\n",
    "        classes = original_classes\n",
    "        results = mmcv.load(WORK_DIR+model_name+\"_\"+dataset_name+\".pkl\")\n",
    "        dataset_size = len(results)\n",
    "    if person_only:\n",
    "        if len(classes) == len(COCO_CLASSES):\n",
    "            classes = [(0,'person')]\n",
    "        elif len(classes) == len(CITYSCAPES_CLASSES):\n",
    "            classes = [(0,'person'),(1,'rider')]\n",
    "    \n",
    "        \n",
    "    return results,classes,dataset_size\n",
    "\n",
    "def gather_results(model_dict: Dict[str,Tuple[str,str,str]], score_thr: float, person_only: bool, result_type='bbox'):\n",
    "    #model_dict = model_dict.items()\n",
    "    ensemble_results = {}\n",
    "    dataset_compatible = -1\n",
    "    label_type = []\n",
    "    for i, (name, (config,checkpoint,download_link)) in enumerate(model_dict):\n",
    "        if not path.exists(checkpoint):\n",
    "            print(\"Downloading\",name)\n",
    "            request.urlretrieve(download_link,checkpoint)\n",
    "            print(\"Finished downloading\",name)\n",
    "        print(\"Loading inference results from model:\",name)\n",
    "        ensemble_results[i],classes,dataset_size = gather_results_from_model(name, (config,checkpoint), score_thr, person_only, result_type)\n",
    "        label_type.append(len(classes))\n",
    "        if dataset_compatible < 0 or dataset_compatible == dataset_size:\n",
    "            dataset_compatible = dataset_size\n",
    "        else:\n",
    "            raise(Exception(\"Dataset sizes are not compatible\"))\n",
    "    return ensemble_results,classes,dataset_compatible\n",
    "\n",
    "def generate_dataset(dataset,model_dict,ensemble_results, labels: List[str], dataset_size, score_thr, threshold, ensemble_method):\n",
    "    #ensemble_results[model][image][bbox or segm][label][instance]\n",
    "    final_results = []\n",
    "    n_models = len(ensemble_results)\n",
    "    #Iterate over all the images\n",
    "    for img in tqdm(range(0,dataset_size)):\n",
    "        bbox_group = []\n",
    "        segm_group = []\n",
    "        ensemble_group = []\n",
    "        img_results = []\n",
    "\n",
    "        #Iterate over all the labels\n",
    "        for (label_nr,label) in labels:\n",
    "            bbox_results = []\n",
    "            segm_results = []\n",
    "            ensemble_res = []\n",
    "            #Create a matrix of already used instances\n",
    "            used_instances = []\n",
    "            for cur_model in range(0,len(ensemble_results)):\n",
    "                used_instances.insert(cur_model,[False]*len(ensemble_results[cur_model][img][0][label_nr]))\n",
    "                \n",
    "            #Iterate over all the models for a certain label and a certain image\n",
    "            for cur_model in range(0,len(ensemble_results)):\n",
    "                #Iterate over the current model's results on a certain label on a certain image\n",
    "                for cur_instance in range(0,len(ensemble_results[cur_model][img][0][label_nr])):\n",
    "                    if not used_instances[cur_model][cur_instance] and ensemble_results[cur_model][img][0][label_nr][cur_instance][4] >= CREDIBILITY_THRESHOLD:\n",
    "                    #if not used_instances[cur_model][cur_instance] and ensemble_results[cur_model][img][0][label_nr][cur_instance][4] >= model_dict[cur_model][1][3]:\n",
    "                        used_instances[cur_model][cur_instance] = True\n",
    "                        cur_instance_group = [None for w in range(0,len(ensemble_results))]\n",
    "                        cur_instance_group[cur_model] = (ensemble_results[cur_model][img][0][label_nr][cur_instance],\n",
    "                                                         ensemble_results[cur_model][img][1][label_nr][cur_instance])\n",
    "                        #Iterate over all the other models\n",
    "                        for comp_model in range(cur_model+1,len(ensemble_results)):\n",
    "                            deviations = []\n",
    "                            #Iterate over each of the other model's results\n",
    "                            for comp_instance in range(0,len(ensemble_results[comp_model][img][0][label_nr])):\n",
    "                                if ensemble_results[comp_model][img][0][label_nr][comp_instance][4] >= CREDIBILITY_THRESHOLD:\n",
    "                                #if ensemble_results[comp_model][img][0][label_nr][comp_instance][4] >= model_dict[comp_model][1][3]:\n",
    "                                    if not used_instances[comp_model][comp_instance]:\n",
    "                                        #cur_iou = IoU(ensemble_results[cur_model][img][0][label_nr][cur_instance],ensemble_results[comp_model][img][0][label_nr][comp_instance])\n",
    "                                        boxA = ensemble_results[cur_model][img][0][label_nr][cur_instance]\n",
    "                                        boxB = ensemble_results[comp_model][img][0][label_nr][comp_instance]\n",
    "                                        xA = int(round(min(boxA[0], boxB[0])))\n",
    "                                        yA = int(round(min(boxA[1], boxB[1])))\n",
    "                                        xB = int(round(max(boxA[2], boxB[2])))\n",
    "                                        yB = int(round(max(boxA[3], boxB[3])))\n",
    "                                        cur_iou = IoU_Mask(mask_util.decode(ensemble_results[cur_model][img][1][label_nr][cur_instance])[yA:yB,xA:xB],\n",
    "                                                           mask_util.decode(ensemble_results[comp_model][img][1][label_nr][comp_instance])[yA:yB,xA:xB])\n",
    "                                        \n",
    "                                    else:\n",
    "                                        cur_iou = 0.0\n",
    "                                    deviations.append(cur_iou)\n",
    "                            #Check if the max iou is within the threshold and add the new instance to the group\n",
    "                            if len(deviations) > 0:\n",
    "                                pos = max(range(len(deviations)), key=deviations.__getitem__)\n",
    "                                if deviations[pos] >= threshold:\n",
    "                                    #Guarantee this instance isn't used again\n",
    "                                    used_instances[comp_model][pos] = True\n",
    "                                    cur_instance_group[comp_model] = (ensemble_results[comp_model][img][0][label_nr][pos],\n",
    "                                                                      ensemble_results[comp_model][img][1][label_nr][pos])\n",
    "                        \n",
    "                        count = 0\n",
    "                        for instance_i in cur_instance_group:\n",
    "                            if instance_i:\n",
    "                                count += 1\n",
    "                                \n",
    "                        # Assuming an instance group is viable if most of the networks identified it\n",
    "                        if (count >= (n_models/2) + VIABLE_COUNTABILITY and not ensemble_method == \"bitwise_and\") or \\\n",
    "                           (count == n_models and ensemble_method == \"bitwise_and\"):\n",
    "                            bbox = np.array([0.0]*5)\n",
    "                            for model_result in range(0,len(cur_instance_group)):\n",
    "                                if not cur_instance_group[model_result] is None:\n",
    "                                    bbox = np.add(bbox,cur_instance_group[model_result][0])\n",
    "                            bbox = (bbox/count)\n",
    "                            confidence = bbox[4]\n",
    "                            bbox = bbox.astype(int)\n",
    "                            bbox[0:3] = np.around(bbox[0:3])\n",
    "                            bbox_y = (bbox[3]-bbox[1]).astype(int)\n",
    "                            bbox_x = (bbox[2]-bbox[0]).astype(int)\n",
    "                            mask = np.zeros((bbox_y,bbox_x),dtype=int)\n",
    "                            img_size = (0,0)\n",
    "                            if ensemble_method == \"average\":\n",
    "                                for model_result in range(0,len(cur_instance_group)):\n",
    "                                    if not cur_instance_group[model_result] is None:\n",
    "                                        decoded_mask = mask_util.decode(cur_instance_group[model_result][1])\n",
    "                                        mask = mask+decoded_mask[bbox[1]:bbox[1]+bbox_y,bbox[0]:bbox[0]+bbox_x].astype(int)\n",
    "                                        img_size = decoded_mask.shape\n",
    "                                acceptability = max(1,count/2 + AVERAGE_ACCEPTABILITY)\n",
    "                                mask = mask >= acceptability\n",
    "                            elif ensemble_method == \"weighted_average\":\n",
    "                                total_confidence = 0.0\n",
    "                                for model_result in range(0,len(cur_instance_group)):\n",
    "                                    if not cur_instance_group[model_result] is None:\n",
    "                                        decoded_mask = mask_util.decode(cur_instance_group[model_result][1])\n",
    "                                        mask = mask+(decoded_mask[bbox[1]:bbox[1]+bbox_y,bbox[0]:bbox[0]+bbox_x].astype(int) * confidence)\n",
    "                                        total_confidence += confidence\n",
    "                                        img_size = decoded_mask.shape\n",
    "                                mask = mask >= AVERAGE_ACCEPTABILITY_2 * total_confidence\n",
    "                            elif ensemble_method == \"bitwise_or\":\n",
    "                                for model_result in range(0,len(cur_instance_group)):\n",
    "                                    if not cur_instance_group[model_result] is None:\n",
    "                                        decoded_mask = mask_util.decode(cur_instance_group[model_result][1])\n",
    "                                        mask = mask+decoded_mask[bbox[1]:bbox[1]+bbox_y,bbox[0]:bbox[0]+bbox_x].astype(int)\n",
    "                                        img_size = decoded_mask.shape\n",
    "                                mask = mask > 0.0\n",
    "                            elif ensemble_method == \"bitwise_and\":\n",
    "                                for model_result in range(0,len(cur_instance_group)):\n",
    "                                    decoded_mask = mask_util.decode(cur_instance_group[model_result][1])\n",
    "                                    mask = mask+decoded_mask[bbox[1]:bbox[1]+bbox_y,bbox[0]:bbox[0]+bbox_x].astype(int)\n",
    "                                    img_size = decoded_mask.shape\n",
    "                                mask = mask == float(n_models)\n",
    "                            segmentation = np.zeros(img_size).astype(bool)\n",
    "                            segmentation[bbox[1]:bbox[1]+bbox_y,bbox[0]:bbox[0]+bbox_x] = mask\n",
    "                            bbox = bbox.astype(float)\n",
    "                            bbox[4] = confidence\n",
    "                            img_results.append((np.array(bbox),mask_util.encode(np.asfortranarray(segmentation)),np.array(cur_instance_group)))\n",
    "                            #bbox_results.append(np.array(bbox))\n",
    "                            #segm_results.append(mask_util.encode(np.asfortranarray(segmentation)))\n",
    "                            #ensemble_res.append(np.array(cur_instance_group))\n",
    "            #if not bbox_results is None:\n",
    "                #np.append(bbox_results,np.array([]))  \n",
    "            #bbox_group.append(np.array(bbox_results).reshape(-1,5)) \n",
    "            #segm_group.append(segm_results)\n",
    "            #ensemble_group.append(ensemble_res)\n",
    "        #final_results.append((bbox_group,segm_group,ensemble_group))    \n",
    "        final_results.append(img_results)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def get_dataset():\n",
    "    test_cfg = Config.fromfile(test_config)\n",
    "    # in case the test dataset is concatenated\n",
    "    samples_per_gpu = 1\n",
    "    if isinstance(test_cfg.data.test, dict):\n",
    "        test_cfg.data.test.test_mode = True\n",
    "        samples_per_gpu = test_cfg.data.test.pop('samples_per_gpu', 1)\n",
    "        if samples_per_gpu > 1:\n",
    "            # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "            test_cfg.data.test.pipeline = replace_ImageToTensor(\n",
    "                test_cfg.data.test.pipeline)\n",
    "    elif isinstance(test_cfg.data.test, list):\n",
    "        for ds_cfg in test_cfg.data.test:\n",
    "            ds_cfg.test_mode = True\n",
    "        samples_per_gpu = max(\n",
    "            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in test_cfg.data.test])\n",
    "        if samples_per_gpu > 1:\n",
    "            for ds_cfg in test_cfg.data.test:\n",
    "                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "    # init distributed env first, since logger depends on the dist info.\n",
    "    distributed = False\n",
    "\n",
    "    #rank, _ = get_dist_info()\n",
    "    # allows not to create\n",
    "    #mmcv.mkdir_or_exist(osp.abspath(args.work_dir))\n",
    "    #timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "    #json_file = osp.join(args.work_dir, f'eval_{timestamp}.json')\n",
    "\n",
    "    # build the dataloader\n",
    "    dataset = build_dataset(test_cfg.data.test)\n",
    "    data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        samples_per_gpu=samples_per_gpu,\n",
    "        workers_per_gpu=test_cfg.data.workers_per_gpu,\n",
    "        dist=distributed,\n",
    "        shuffle=False)\n",
    "    \n",
    "    return dataset, data_loader\n",
    "\n",
    "\n",
    "def get_results(model_dict, score_thr, person_only, ensemble_method, dataset, result_type='segm'):\n",
    "    ensemble_results,classes,dataset_size = gather_results(model_dict,score_thr,person_only,result_type)\n",
    "    results = generate_dataset(dataset,model_dict,ensemble_results,classes,dataset_size,score_thr,DEVIATION_THRESHOLD,ensemble_method)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43aa72f1-72ea-4d1b-93ad-44c03e8561b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def match_instances(image,\n",
    "                    gt_segm,\n",
    "                    dt_segm,\n",
    "                    img_size,\n",
    "                    threshold):\n",
    "    matched = []\n",
    "    dt_unmatched = []\n",
    "    used = [False for i in range(0,len(dt_segm))]\n",
    "    \n",
    "    #Case Empty\n",
    "    if len(gt_segm) == 0 or len(dt_segm) == 0:\n",
    "        matched = []\n",
    "        dt_unmatched = []\n",
    "        for i in range(0,len(dt_segm)):\n",
    "            return_group = []\n",
    "            for x in dt_segm[i][2]:\n",
    "                if x is None:\n",
    "                    return_group.append(np.zeros(img_size,dtype=np.uint8))\n",
    "                else:\n",
    "                    return_group.append(mask_util.decode(x[1]))\n",
    "            pred_stack = np.dstack(return_group)    \n",
    "            dt_unmatched.append((pred_stack,np.zeros(img_size,dtype=np.uint8)))\n",
    "        return matched, dt_unmatched\n",
    "\n",
    "    crowds = []\n",
    "    for gt in range(0,len(gt_segm)):\n",
    "        if gt_segm[gt]['iscrowd'] == 1:\n",
    "            crowds.append(gt)\n",
    "        else:\n",
    "            gt_bbox = gt_segm[gt]['bbox'].copy()\n",
    "            gt_bbox[2] = gt_bbox[0]+gt_bbox[2]\n",
    "            gt_bbox[3] = gt_bbox[1]+gt_bbox[3]\n",
    "            gt_segm_decoded = mask_util.decode(gt_segm[gt]['segmentation'])\n",
    "            deviations = []\n",
    "            for dt in range(0,len(dt_segm)):\n",
    "                if not used[dt]:\n",
    "                    dt_bbox = dt_segm[dt][0].copy()\n",
    "                    dt_bbox[2] = dt_bbox[0]+dt_bbox[2]\n",
    "                    dt_bbox[3] = dt_bbox[1]+dt_bbox[3]\n",
    "                    #cur_iou = IoU(gt_bbox,dt_bbox)\n",
    "\n",
    "                    xA = int(round(min(gt_bbox[0], dt_bbox[0])))\n",
    "                    yA = int(round(min(gt_bbox[1], dt_bbox[1])))\n",
    "                    xB = int(round(max(gt_bbox[2], dt_bbox[2])))\n",
    "                    yB = int(round(max(gt_bbox[3], dt_bbox[3])))\n",
    "                    cur_iou = IoU_Mask(gt_segm_decoded[yA:yB,xA:xB],\n",
    "                                       mask_util.decode(dt_segm[dt][1])[yA:yB,xA:xB])\n",
    "                    deviations.append(cur_iou)\n",
    "                else:\n",
    "                    deviations.append(0.0)\n",
    "            pos = max(range(len(deviations)), key=deviations.__getitem__)\n",
    "            if deviations[pos] >= threshold:\n",
    "                #Guarantee this instance isn't used again\n",
    "                used[pos] = True\n",
    "                return_group = []\n",
    "                for x in dt_segm[pos][2]:\n",
    "                    if x is None:\n",
    "                        return_group.append(np.zeros(img_size,dtype=np.uint8))\n",
    "                    else:\n",
    "                        return_group.append(mask_util.decode(x[1]))\n",
    "                pred_stack = np.dstack(return_group)\n",
    "                matched.append((pred_stack,mask_util.decode(gt_segm[gt]['segmentation'])))\n",
    "\n",
    "    for i in range(0,len(used)):\n",
    "        if not used[i]:\n",
    "            dt_segm_decoded = mask_util.decode(dt_segm[dt][1])\n",
    "            dt_bbox = dt_segm[dt][0].copy()\n",
    "            dt_bbox[2] = dt_bbox[0]+dt_bbox[2]\n",
    "            dt_bbox[3] = dt_bbox[1]+dt_bbox[3]\n",
    "            xA,xB,yA,yB = int(round(dt_bbox[0])),int(round(dt_bbox[2])),int(round(dt_bbox[1])),int(round(dt_bbox[3]))\n",
    "            deviations = []\n",
    "            for c in crowds:\n",
    "                intersection = np.logical_and(mask_util.decode(gt_segm[c]['segmentation']),\n",
    "                                              dt_segm_decoded)\n",
    "                cur_iou = IoU_Mask(dt_segm_decoded[yA:yB,xA:xB],\n",
    "                                   intersection[yA:yB,xA:xB])\n",
    "                \n",
    "                fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(15,15))\n",
    "                ax=ax.flat\n",
    "                ax[0].set_title(\"Original Image\")  # set title\n",
    "                ax[0].imshow(image)\n",
    "                ax[1].set_title(\"DT SEGM\")  # set title\n",
    "                ax[1].imshow(dt_segm_decoded,cmap='gray',vmin=0,vmax=1)\n",
    "                ax[2].set_title(\"Intersection\")  # set title\n",
    "                ax[2].imshow(intersection,cmap='gray',vmin=0,vmax=1)\n",
    "                ax[3].set_title(\"Crowd\")  # set title\n",
    "                ax[3].imshow(mask_util.decode(gt_segm[c]['segmentation']),cmap='gray',vmin=0,vmax=1)\n",
    "                fig.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                deviations.append(cur_iou)\n",
    "            if len(crowds) != 0:\n",
    "                pos = max(range(len(deviations)), key=deviations.__getitem__)\n",
    "                if deviations[pos] < threshold:\n",
    "                    return_group = []\n",
    "                    for x in dt_segm[i][2]:\n",
    "                        if x is None:\n",
    "                            return_group.append(np.zeros(img_size,dtype=np.uint8))\n",
    "                        else:\n",
    "                            return_group.append(mask_util.decode(x[1]))\n",
    "                    pred_stack = np.dstack(return_group)    \n",
    "                    dt_unmatched.append((pred_stack,np.zeros(img_size,dtype=np.uint8)))\n",
    "            else:\n",
    "                return_group = []\n",
    "                for x in dt_segm[i][2]:\n",
    "                    if x is None:\n",
    "                        return_group.append(np.zeros(img_size,dtype=np.uint8))\n",
    "                    else:\n",
    "                        return_group.append(mask_util.decode(x[1]))\n",
    "                pred_stack = np.dstack(return_group)    \n",
    "                dt_unmatched.append((pred_stack,np.zeros(img_size,dtype=np.uint8)))\n",
    "\n",
    "    return matched, dt_unmatched\n",
    "\n",
    "def matcher_no_img(dt,dataset,img=False):\n",
    "    result_dataset = []\n",
    "    gt = dataset.gt_return()\n",
    "    for i in tqdm(range(0,len(dataset))):        \n",
    "        filename = dataset[i]['img_metas'][0].data['filename']\n",
    "        size = dataset[i]['img_metas'][0].data['ori_shape']\n",
    "        \n",
    "        imgId = int(filename.split('/')[-1].split('.')[0])\n",
    "        cur_dt = dt[i]\n",
    "        cur_gt = gt[imgId]\n",
    "        image = Image.open(filename)\n",
    "        img_array = np.asarray(image)\n",
    "        \n",
    "        matched, unmatched = match_instances(image,cur_gt,cur_dt,(size[0],size[1]),DEVIATION_THRESHOLD_EVAL)\n",
    "        for m in matched:\n",
    "            res = m[0].copy()\n",
    "            result_dataset.append((filename,True,res,m[1]))\n",
    "        for u in unmatched:\n",
    "            res = u[0].copy()\n",
    "            result_dataset.append((filename,False,res,u[1]))\n",
    "            \n",
    "    return result_dataset\n",
    "\n",
    "def matcher(dt,dataset):\n",
    "    result_dataset = []\n",
    "    gt = dataset.gt_return()\n",
    "    for i in tqdm(range(0,len(dataset))):\n",
    "        filename = dataset[i]['img_metas'][0].data['filename']\n",
    "        size = dataset[i]['img_metas'][0].data['ori_shape']\n",
    "        \n",
    "        imgId = int(filename.split('/')[-1].split('.')[0])\n",
    "        cur_dt = dt[i]\n",
    "        cur_gt = gt[imgId]\n",
    "        image = Image.open(filename)\n",
    "        img_array = np.asarray(image)\n",
    "        \n",
    "        matched, unmatched = match_instances(cur_gt,cur_dt,(size[0],size[1]),DEVIATION_THRESHOLD_EVAL)\n",
    "        for m in matched:\n",
    "            res = np.dstack((img_array,m[0]))\n",
    "            result_dataset.append((filename,True,res,m[1]))\n",
    "        for u in unmatched:\n",
    "            res = np.dstack((img_array,u[0]))\n",
    "            result_dataset.append((filename,res,u[1]))\n",
    "            \n",
    "    return result_dataset\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0da963-e344-4faf-9c08-8d18f3e52ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading inference results from model: mask_rcnn_X-101-64x4d-FPN\n",
      "load checkpoint from local path: checkpoints/mask_rcnn_x101_64x4d_fpn_mstrain-poly_3x_coco_20210526_120447-c376f129.pth\n",
      "Loading inference results from model: cascade_mask_rcnn_X-101-64x4d-FPN\n",
      "load checkpoint from local path: checkpoints/cascade_mask_rcnn_x101_64x4d_fpn_mstrain_3x_coco_20210719_210311-d3e64ba0.pth\n",
      "Loading inference results from model: hybrid_task_cascade_mask_rcnn_X-101-64x4d-FPN\n",
      "load checkpoint from local path: checkpoints/htc_x101_64x4d_fpn_dconv_c3-c5_mstrain_400_1400_16x1_20e_coco_20200312-946fd751.pth\n",
      "Loading inference results from model: gcnet_X-101-FPN_DCN_Cascade_Mask_GC(c3-c5,r4)\n",
      "load checkpoint from local path: checkpoints/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_dconv_c3-c5_r4_gcb_c3-c5_1x_coco_20210615_161851-720338ec.pth\n",
      "Loading inference results from model: scnet_X-101-64x4d-FPN\n",
      "load checkpoint from local path: checkpoints/scnet_x101_64x4d_fpn_20e_coco-fb09dec9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [01:19<00:00, 62.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "dataset, data_loader = get_dataset()\n",
    "results = get_results(model_dict, CREDIBILITY_THRESHOLD,True,ENSEMBLE_METHOD,dataset,result_type='segm')\n",
    "#final_results = match_instances(dataset.return_gt(),results)\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b8340-e722-4109-9b5e-2b0c041b8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = matcher_no_img(results,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f08381-9101-4286-ae9e-217832dbb77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in range(0,len(final_results)):\n",
    "    if not final_results[img][1]:\n",
    "        print(final_results[img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b764471-8a94-4a7c-808e-b47c357c88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import bz2\n",
    "\n",
    "with open('work_dirs/dataset_generation/dataset_no_img.pkl', 'wb') as handle:\n",
    "    #compressed_file = bz2.BZ2File(handle, 'w')\n",
    "    pickle.dump(final_results, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "899d1f5c-02ba-4e76-89f3-8579008deebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9412"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec0dc4-a052-49d2-be7a-b52f0b720ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
