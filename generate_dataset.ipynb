{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb0fb4-4f59-480b-aacd-3811ac29805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average / weighted_average / bitwise_or\n",
    "ENSEMBLE_METHOD = \"bitwise_or\"\n",
    "VIABLE_COUNTABILITY = 0\n",
    "AVERAGE_ACCEPTABILITY = 0\n",
    "# Minimum value of credibility per mask\n",
    "CREDIBILITY_THRESHOLD = 0.5\n",
    "# Minimum IoU in order to group instances together\n",
    "DEVIATION_THRESHOLD = 0.5\n",
    "# Minimum IoU in order to compare instances while evaluating \n",
    "DEVIATION_THRESHOLD_EVAL = 0.5\n",
    "#For Weighted Average\n",
    "AVERAGE_ACCEPTABILITY_2 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30065b-3897-4937-a569-dba032df03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_dict = []\n",
    "model_dict.append(('hybrid_task_cascade_mask_rcnn_X-101-64x4d-FPN',('configs/htc/htc_x101_64x4d_fpn_dconv_c3-c5_mstrain_400_1400_16x1_20e_coco.py',\n",
    "                                                                'checkpoints/htc_x101_64x4d_fpn_dconv_c3-c5_mstrain_400_1400_16x1_20e_coco_20200312-946fd751.pth',\n",
    "                                                                'https://download.openmmlab.com/mmdetection/v2.0/htc/htc_x101_64x4d_fpn_dconv_c3-c5_mstrain_400_1400_16x1_20e_coco/htc_x101_64x4d_fpn_dconv_c3-c5_mstrain_400_1400_16x1_20e_coco_20200312-946fd751.pth')))\n",
    "model_dict.append(('detectors_htc_r101_20e_coco',('configs/detectors/detectors_htc_r101_20e_coco.py',\n",
    "                                                  'checkpoints/detectors_htc_r101_20e_coco_20210419_203638-348d533b.pth',\n",
    "                                                  'https://download.openmmlab.com/mmdetection/v2.0/detectors/detectors_htc_r101_20e_coco/detectors_htc_r101_20e_coco_20210419_203638-348d533b.pth')))\n",
    "model_dict.append(('cascade_mask_rcnn_X-101-64x4d-FPN',('configs/cascade_rcnn/cascade_mask_rcnn_x101_64x4d_fpn_mstrain_3x_coco.py',\n",
    "                                                        'checkpoints/cascade_mask_rcnn_x101_64x4d_fpn_mstrain_3x_coco_20210719_210311-d3e64ba0.pth',\n",
    "                                                        'https://download.openmmlab.com/mmdetection/v2.0/cascade_rcnn/cascade_mask_rcnn_x101_64x4d_fpn_mstrain_3x_coco/cascade_mask_rcnn_x101_64x4d_fpn_mstrain_3x_coco_20210719_210311-d3e64ba0.pth')))\n",
    "model_dict.append(('cascade_mask_rcnn_x101_32x4d_fpn_dconv_c3-c5_1x_coco',('configs/dcn/cascade_mask_rcnn_x101_32x4d_fpn_dconv_c3-c5_1x_coco.py',\n",
    "                                                                           'checkpoints/cascade_mask_rcnn_x101_32x4d_fpn_dconv_c3-c5_1x_coco-e75f90c8.pth',\n",
    "                                                                           'https://download.openmmlab.com/mmdetection/v2.0/dcn/cascade_mask_rcnn_x101_32x4d_fpn_dconv_c3-c5_1x_coco/cascade_mask_rcnn_x101_32x4d_fpn_dconv_c3-c5_1x_coco-e75f90c8.pth')))\n",
    "model_dict.append(('gcnet_X-101-FPN_DCN_Cascade_Mask_GC(c3-c5,r4)',('configs/gcnet/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_dconv_c3-c5_r4_gcb_c3-c5_1x_coco.py',\n",
    "                                                                    'checkpoints/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_dconv_c3-c5_r4_gcb_c3-c5_1x_coco_20210615_161851-720338ec.pth',\n",
    "                                                                    'https://download.openmmlab.com/mmdetection/v2.0/gcnet/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_dconv_c3-c5_r4_gcb_c3-c5_1x_coco/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_dconv_c3-c5_r4_gcb_c3-c5_1x_coco_20210615_161851-720338ec.pth')))\n",
    "\n",
    "#test_config = 'configs/common/mstrain-poly_3x_coco_instance.py'\n",
    "test_config = 'configs/common/mstrain-poly_3x_3000_coco_instance.py'\n",
    "#test_config = 'configs/_base_/datasets/cityscapes_instance.py'\n",
    "#test_config = 'configs/common/mstrain-poly_3x_coco_instance_train.py'\n",
    "dataset_name = os.path.splitext(test_config)[0].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f59362-f412-4c4e-a32b-7fe9411a2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def IoU(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "def IoU_Mask(maskA, maskB):\n",
    "    intersection = np.logical_and(maskA, maskB).astype(np.uint8)\n",
    "    union = np.logical_or(maskA, maskB).astype(np.uint8)\n",
    "    iou = np.sum(intersection)/np.sum(union)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f2d1fc-2c72-440b-a394-2ca4615d589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "from mmdet.datasets import build_dataset, build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmdet.apis import single_gpu_test\n",
    "from mmdet.models import build_detector\n",
    "from mmcv.runner import load_checkpoint\n",
    "from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n",
    "from typing import List, Tuple, Union, Dict\n",
    "from torch import nn\n",
    "from os import path\n",
    "from urllib import request\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import mmcv\n",
    "import os.path as osp\n",
    "import pycocotools.mask as mask_util\n",
    "\n",
    "COCO_CLASSES = ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "                'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n",
    "                'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
    "                'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',\n",
    "                'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "                'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n",
    "                'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "                'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "                'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\n",
    "                'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "                'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',\n",
    "                'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "                'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\n",
    "                'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush')\n",
    "\n",
    "CITYSCAPES_CLASSES = ('person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle',\n",
    "                      'bicycle')\n",
    "\n",
    "WORK_DIR = \"work_dirs/ensemble_results/\"\n",
    "\n",
    "def inference_on_dataset(model_info):\n",
    "    config, checkpoint = model_info\n",
    "    \n",
    "    cfg = Config.fromfile(config)\n",
    "\n",
    "    if cfg.get('custom_imports', None):\n",
    "        from mmcv.utils import import_modules_from_strings\n",
    "        import_modules_from_strings(**cfg['custom_imports'])\n",
    "    # set cudnn_benchmark\n",
    "    if cfg.get('cudnn_benchmark', False):\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    cfg.model.pretrained = None\n",
    "    if cfg.model.get('neck'):\n",
    "        if isinstance(cfg.model.neck, list):\n",
    "            for neck_cfg in cfg.model.neck:\n",
    "                if neck_cfg.get('rfp_backbone'):\n",
    "                    if neck_cfg.rfp_backbone.get('pretrained'):\n",
    "                        neck_cfg.rfp_backbone.pretrained = None\n",
    "        elif cfg.model.neck.get('rfp_backbone'):\n",
    "            if cfg.model.neck.rfp_backbone.get('pretrained'):\n",
    "                cfg.model.neck.rfp_backbone.pretrained = None\n",
    "                \n",
    "    test_cfg = Config.fromfile(test_config)\n",
    "    # in case the test dataset is concatenated\n",
    "    samples_per_gpu = 1\n",
    "    if isinstance(cfg.data.test, dict):\n",
    "        test_cfg.data.test.test_mode = True\n",
    "        samples_per_gpu = test_cfg.data.test.pop('samples_per_gpu', 1)\n",
    "        if samples_per_gpu > 1:\n",
    "            # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "            test_cfg.data.test.pipeline = replace_ImageToTensor(\n",
    "                test_cfg.data.test.pipeline)\n",
    "    elif isinstance(test_cfg.data.test, list):\n",
    "        for ds_cfg in test_cfg.data.test:\n",
    "            ds_cfg.test_mode = True\n",
    "        samples_per_gpu = max(\n",
    "            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in test_cfg.data.test])\n",
    "        if samples_per_gpu > 1:\n",
    "            for ds_cfg in test_cfg.data.test:\n",
    "                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "    # init distributed env first, since logger depends on the dist info.\n",
    "    distributed = False\n",
    "\n",
    "    #rank, _ = get_dist_info()\n",
    "    # allows not to create\n",
    "    #mmcv.mkdir_or_exist(osp.abspath(args.work_dir))\n",
    "    #timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "    #json_file = osp.join(args.work_dir, f'eval_{timestamp}.json')\n",
    "\n",
    "    # build the dataloader\n",
    "    dataset = build_dataset(test_cfg.data.test)\n",
    "    \n",
    "    data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        samples_per_gpu=samples_per_gpu,\n",
    "        workers_per_gpu=test_cfg.data.workers_per_gpu,\n",
    "        dist=distributed,\n",
    "        shuffle=False)\n",
    "\n",
    "    # build the model and load checkpoint\n",
    "    cfg.model.train_cfg = None\n",
    "    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "    fp16_cfg = cfg.get('fp16', None)\n",
    "    if fp16_cfg is not None:\n",
    "        wrap_fp16_model(model)\n",
    "    checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n",
    "    #if args.fuse_conv_bn:\n",
    "        #model = fuse_conv_bn(model)\n",
    "    # old versions did not save class info in checkpoints, this walkaround is\n",
    "    # for backward compatibility\n",
    "    if 'CLASSES' in checkpoint.get('meta', {}):\n",
    "        model.CLASSES = checkpoint['meta']['CLASSES']\n",
    "    else:\n",
    "        model.CLASSES = dataset.CLASSES\n",
    "\n",
    "    #classes = model.CLASSES.copy()\n",
    "    classes = list(enumerate(model.CLASSES)).copy()\n",
    "    model = MMDataParallel(model, device_ids=[0])\n",
    "    outputs = single_gpu_test(model, data_loader, None, None, None)\n",
    "    \n",
    "    print(len(dataset))\n",
    "    \n",
    "    return outputs, classes, len(dataset)\n",
    "\n",
    "\n",
    "def gather_results_from_model(model_name: str, model_info: Tuple[str,str], score_thr:float, person_only:bool , result_type = 'bbox'):\n",
    "    if not osp.exists(WORK_DIR+model_name+\"_\"+dataset_name+\".pkl\"):\n",
    "        mmcv.mkdir_or_exist(osp.abspath(WORK_DIR))\n",
    "        results, original_classes, dataset_size = inference_on_dataset(model_info)\n",
    "        classes = original_classes\n",
    "        mmcv.dump(results, WORK_DIR+model_name+\"_\"+dataset_name+\".pkl\")\n",
    "    else:\n",
    "        config,checkpoint = model_info\n",
    "        cfg = Config.fromfile(config)\n",
    "        model = build_detector(cfg.model)\n",
    "        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n",
    "        if 'CLASSES' in checkpoint.get('meta', {}):\n",
    "            model.CLASSES = checkpoint['meta']['CLASSES']\n",
    "        else:\n",
    "            model.CLASSES = dataset.CLASSES\n",
    "        original_classes = list(enumerate(model.CLASSES)).copy()\n",
    "        classes = original_classes\n",
    "        results = mmcv.load(WORK_DIR+model_name+\"_\"+dataset_name+\".pkl\")\n",
    "        dataset_size = len(results)\n",
    "    if person_only:\n",
    "        if len(classes) == len(COCO_CLASSES):\n",
    "            classes = [(0,'person')]\n",
    "        elif len(classes) == len(CITYSCAPES_CLASSES):\n",
    "            classes = [(0,'person'),(1,'rider')]\n",
    "    \n",
    "        \n",
    "    return results,classes,dataset_size\n",
    "\n",
    "def gather_results(model_dict: Dict[str,Tuple[str,str,str]], score_thr: float, person_only: bool, result_type='bbox'):\n",
    "    #model_dict = model_dict.items()\n",
    "    ensemble_results = {}\n",
    "    dataset_compatible = -1\n",
    "    label_type = []\n",
    "    for i, (name, (config,checkpoint,download_link)) in enumerate(model_dict):\n",
    "        if not path.exists(checkpoint):\n",
    "            print(\"Downloading\",name)\n",
    "            request.urlretrieve(download_link,checkpoint)\n",
    "            print(\"Finished downloading\",name)\n",
    "        print(\"Loading inference results from model:\",name)\n",
    "        ensemble_results[i],classes,dataset_size = gather_results_from_model(name, (config,checkpoint), score_thr, person_only, result_type)\n",
    "        label_type.append(len(classes))\n",
    "        if dataset_compatible < 0 or dataset_compatible == dataset_size:\n",
    "            dataset_compatible = dataset_size\n",
    "        else:\n",
    "            raise(Exception(\"Dataset sizes are not compatible\"))\n",
    "    return ensemble_results,classes,dataset_compatible\n",
    "\n",
    "def generate_dataset(dataset,model_dict,ensemble_results, labels: List[str], dataset_size, score_thr, threshold, ensemble_method):\n",
    "    #ensemble_results[model][image][bbox or segm][label][instance]\n",
    "    final_results = []\n",
    "    n_models = len(ensemble_results)\n",
    "    #Iterate over all the images\n",
    "    for img in tqdm(range(0,dataset_size)):\n",
    "        bbox_group = []\n",
    "        segm_group = []\n",
    "        ensemble_group = []\n",
    "        img_results = []\n",
    "\n",
    "        #Iterate over all the labels\n",
    "        for (label_nr,label) in labels:\n",
    "            bbox_results = []\n",
    "            segm_results = []\n",
    "            ensemble_res = []\n",
    "            #Create a matrix of already used instances\n",
    "            used_instances = []\n",
    "            for cur_model in range(0,len(ensemble_results)):\n",
    "                used_instances.insert(cur_model,[False]*len(ensemble_results[cur_model][img][0][label_nr]))\n",
    "                \n",
    "            #Iterate over all the models for a certain label and a certain image\n",
    "            for cur_model in range(0,len(ensemble_results)):\n",
    "                #Iterate over the current model's results on a certain label on a certain image\n",
    "                for cur_instance in range(0,len(ensemble_results[cur_model][img][0][label_nr])):\n",
    "                    if not used_instances[cur_model][cur_instance] and ensemble_results[cur_model][img][0][label_nr][cur_instance][4] >= CREDIBILITY_THRESHOLD:\n",
    "                    #if not used_instances[cur_model][cur_instance] and ensemble_results[cur_model][img][0][label_nr][cur_instance][4] >= model_dict[cur_model][1][3]:\n",
    "                        used_instances[cur_model][cur_instance] = True\n",
    "                        cur_instance_group = [None for w in range(0,len(ensemble_results))]\n",
    "                        cur_instance_group[cur_model] = (ensemble_results[cur_model][img][0][label_nr][cur_instance],\n",
    "                                                         ensemble_results[cur_model][img][1][label_nr][cur_instance])\n",
    "                        #Iterate over all the other models\n",
    "                        for comp_model in range(cur_model+1,len(ensemble_results)):\n",
    "                            deviations = []\n",
    "                            #Iterate over each of the other model's results\n",
    "                            for comp_instance in range(0,len(ensemble_results[comp_model][img][0][label_nr])):\n",
    "                                if ensemble_results[comp_model][img][0][label_nr][comp_instance][4] >= CREDIBILITY_THRESHOLD:\n",
    "                                #if ensemble_results[comp_model][img][0][label_nr][comp_instance][4] >= model_dict[comp_model][1][3]:\n",
    "                                    if not used_instances[comp_model][comp_instance]:\n",
    "                                        #cur_iou = IoU(ensemble_results[cur_model][img][0][label_nr][cur_instance],ensemble_results[comp_model][img][0][label_nr][comp_instance])\n",
    "                                        boxA = ensemble_results[cur_model][img][0][label_nr][cur_instance]\n",
    "                                        boxB = ensemble_results[comp_model][img][0][label_nr][comp_instance]\n",
    "                                        xA = int(round(min(boxA[0], boxB[0])))\n",
    "                                        yA = int(round(min(boxA[1], boxB[1])))\n",
    "                                        xB = int(round(max(boxA[2], boxB[2])))\n",
    "                                        yB = int(round(max(boxA[3], boxB[3])))\n",
    "                                        cur_iou = IoU_Mask(mask_util.decode(ensemble_results[cur_model][img][1][label_nr][cur_instance])[yA:yB,xA:xB],\n",
    "                                                           mask_util.decode(ensemble_results[comp_model][img][1][label_nr][comp_instance])[yA:yB,xA:xB])\n",
    "                                        \n",
    "                                    else:\n",
    "                                        cur_iou = 0.0\n",
    "                                    deviations.append(cur_iou)\n",
    "                            #Check if the max iou is within the threshold and add the new instance to the group\n",
    "                            if len(deviations) > 0:\n",
    "                                pos = max(range(len(deviations)), key=deviations.__getitem__)\n",
    "                                if deviations[pos] >= threshold:\n",
    "                                    #Guarantee this instance isn't used again\n",
    "                                    used_instances[comp_model][pos] = True\n",
    "                                    cur_instance_group[comp_model] = (ensemble_results[comp_model][img][0][label_nr][pos],\n",
    "                                                                      ensemble_results[comp_model][img][1][label_nr][pos])\n",
    "                        \n",
    "                        count = 0\n",
    "                        for instance_i in cur_instance_group:\n",
    "                            if instance_i:\n",
    "                                count += 1\n",
    "                                \n",
    "                        # Assuming an instance group is viable if most of the networks identified it\n",
    "                        if (count >= (n_models/2) + VIABLE_COUNTABILITY and (not (ensemble_method == \"bitwise_and\")) and (not (ensemble_method == \"bitwise_or\"))) or \\\n",
    "                           (count == n_models and ensemble_method == \"bitwise_and\") or \\\n",
    "                           (ensemble_method == \"bitwise_or\"):\n",
    "                            bbox = np.array([0.0]*5)\n",
    "                            for model_result in range(0,len(cur_instance_group)):\n",
    "                                if not cur_instance_group[model_result] is None:\n",
    "                                    bbox = np.add(bbox,cur_instance_group[model_result][0])\n",
    "                            bbox = (bbox/count)\n",
    "                            confidence = bbox[4]\n",
    "                            bbox = bbox.astype(int)\n",
    "                            bbox[0:3] = np.around(bbox[0:3])\n",
    "                            bbox_y = (bbox[3]-bbox[1]).astype(int)\n",
    "                            bbox_x = (bbox[2]-bbox[0]).astype(int)\n",
    "                            mask = np.zeros((bbox_y,bbox_x),dtype=int)\n",
    "                            img_size = (0,0)\n",
    "                            if ensemble_method == \"average\":\n",
    "                                for model_result in range(0,len(cur_instance_group)):\n",
    "                                    if not cur_instance_group[model_result] is None:\n",
    "                                        decoded_mask = mask_util.decode(cur_instance_group[model_result][1])\n",
    "                                        mask = mask+decoded_mask[bbox[1]:bbox[1]+bbox_y,bbox[0]:bbox[0]+bbox_x].astype(int)\n",
    "                                        img_size = decoded_mask.shape\n",
    "                                acceptability = max(1,count/2 + AVERAGE_ACCEPTABILITY)\n",
    "                                mask = mask >= acceptability\n",
    "                            elif ensemble_method == \"weighted_average\":\n",
    "                                total_confidence = 0.0\n",
    "                                for model_result in range(0,len(cur_instance_group)):\n",
    "                                    if not cur_instance_group[model_result] is None:\n",
    "                                        decoded_mask = mask_util.decode(cur_instance_group[model_result][1])\n",
    "                                        mask = mask+(decoded_mask[bbox[1]:bbox[1]+bbox_y,bbox[0]:bbox[0]+bbox_x].astype(int) * confidence)\n",
    "                                        total_confidence += confidence\n",
    "                                        img_size = decoded_mask.shape\n",
    "                                mask = mask >= AVERAGE_ACCEPTABILITY_2 * total_confidence\n",
    "                            elif ensemble_method == \"bitwise_or\":\n",
    "                                for model_result in range(0,len(cur_instance_group)):\n",
    "                                    if not cur_instance_group[model_result] is None:\n",
    "                                        decoded_mask = mask_util.decode(cur_instance_group[model_result][1])\n",
    "                                        mask = mask+decoded_mask[bbox[1]:bbox[1]+bbox_y,bbox[0]:bbox[0]+bbox_x].astype(int)\n",
    "                                        img_size = decoded_mask.shape\n",
    "                                mask = mask > 0.0\n",
    "                            elif ensemble_method == \"bitwise_and\":\n",
    "                                for model_result in range(0,len(cur_instance_group)):\n",
    "                                    decoded_mask = mask_util.decode(cur_instance_group[model_result][1])\n",
    "                                    mask = mask+decoded_mask[bbox[1]:bbox[1]+bbox_y,bbox[0]:bbox[0]+bbox_x].astype(int)\n",
    "                                    img_size = decoded_mask.shape\n",
    "                                mask = mask == float(n_models)\n",
    "                            segmentation = np.zeros(img_size).astype(bool)\n",
    "                            segmentation[bbox[1]:bbox[1]+bbox_y,bbox[0]:bbox[0]+bbox_x] = mask\n",
    "                            bbox = bbox.astype(float)\n",
    "                            bbox[4] = confidence\n",
    "                            img_results.append((np.array(bbox),mask_util.encode(np.asfortranarray(segmentation)),np.array(cur_instance_group)))\n",
    "                            #bbox_results.append(np.array(bbox))\n",
    "                            #segm_results.append(mask_util.encode(np.asfortranarray(segmentation)))\n",
    "                            #ensemble_res.append(np.array(cur_instance_group))\n",
    "            #if not bbox_results is None:\n",
    "                #np.append(bbox_results,np.array([]))  \n",
    "            #bbox_group.append(np.array(bbox_results).reshape(-1,5)) \n",
    "            #segm_group.append(segm_results)\n",
    "            #ensemble_group.append(ensemble_res)\n",
    "        #final_results.append((bbox_group,segm_group,ensemble_group))    \n",
    "        final_results.append(img_results)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def get_dataset():\n",
    "    test_cfg = Config.fromfile(test_config)\n",
    "    # in case the test dataset is concatenated\n",
    "    samples_per_gpu = 1\n",
    "    if isinstance(test_cfg.data.test, dict):\n",
    "        test_cfg.data.test.test_mode = True\n",
    "        samples_per_gpu = test_cfg.data.test.pop('samples_per_gpu', 1)\n",
    "        if samples_per_gpu > 1:\n",
    "            # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "            test_cfg.data.test.pipeline = replace_ImageToTensor(\n",
    "                test_cfg.data.test.pipeline)\n",
    "    elif isinstance(test_cfg.data.test, list):\n",
    "        for ds_cfg in test_cfg.data.test:\n",
    "            ds_cfg.test_mode = True\n",
    "        samples_per_gpu = max(\n",
    "            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in test_cfg.data.test])\n",
    "        if samples_per_gpu > 1:\n",
    "            for ds_cfg in test_cfg.data.test:\n",
    "                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "\n",
    "    # init distributed env first, since logger depends on the dist info.\n",
    "    distributed = False\n",
    "\n",
    "    #rank, _ = get_dist_info()\n",
    "    # allows not to create\n",
    "    #mmcv.mkdir_or_exist(osp.abspath(args.work_dir))\n",
    "    #timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "    #json_file = osp.join(args.work_dir, f'eval_{timestamp}.json')\n",
    "\n",
    "    # build the dataloader\n",
    "    dataset = build_dataset(test_cfg.data.test)\n",
    "    data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        samples_per_gpu=samples_per_gpu,\n",
    "        workers_per_gpu=test_cfg.data.workers_per_gpu,\n",
    "        dist=distributed,\n",
    "        shuffle=False)\n",
    "    \n",
    "    return dataset, data_loader\n",
    "\n",
    "\n",
    "def get_results(model_dict, score_thr, person_only, ensemble_method, dataset, result_type='segm'):\n",
    "    ensemble_results,classes,dataset_size = gather_results(model_dict,score_thr,person_only,result_type)\n",
    "    results = generate_dataset(dataset,model_dict,ensemble_results,classes,dataset_size,score_thr,DEVIATION_THRESHOLD,ensemble_method)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa72f1-72ea-4d1b-93ad-44c03e8561b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def match_instances(gt_segm,\n",
    "                    dt_segm,\n",
    "                    img_size,\n",
    "                    threshold):\n",
    "    matched = []\n",
    "    dt_unmatched = []\n",
    "    used = [False for i in range(0,len(dt_segm))]\n",
    "    \n",
    "    #Case Empty\n",
    "    if len(gt_segm) == 0 or len(dt_segm) == 0:\n",
    "        \n",
    "        matched = []\n",
    "        dt_unmatched = []\n",
    "        for i in range(0,len(dt_segm)):\n",
    "            return_group = []\n",
    "            for x in dt_segm[i][2]:\n",
    "                if x is None:\n",
    "                    return_group.append(np.zeros(img_size,dtype=np.uint8))\n",
    "                else:\n",
    "                    return_group.append(mask_util.decode(x[1]))\n",
    "            pred_stack = np.dstack(return_group)    \n",
    "            dt_unmatched.append((pred_stack,np.zeros(img_size,dtype=np.uint8)))\n",
    "        return matched, dt_unmatched\n",
    "\n",
    "    crowds = []\n",
    "    for gt in range(0,len(gt_segm)):\n",
    "        if gt_segm[gt]['iscrowd'] == 1:\n",
    "            crowds.append(gt)\n",
    "        else:\n",
    "            gt_bbox = gt_segm[gt]['bbox'].copy()\n",
    "            gt_bbox[2] = gt_bbox[0]+gt_bbox[2]\n",
    "            gt_bbox[3] = gt_bbox[1]+gt_bbox[3]\n",
    "            gt_segm_decoded = mask_util.decode(gt_segm[gt]['segmentation'])\n",
    "            deviations = []\n",
    "            for dt in range(0,len(dt_segm)):\n",
    "                if not used[dt]:\n",
    "                    dt_bbox = dt_segm[dt][0].copy()\n",
    "                    dt_bbox[2] = dt_bbox[0]+dt_bbox[2]\n",
    "                    dt_bbox[3] = dt_bbox[1]+dt_bbox[3]\n",
    "                    #cur_iou = IoU(gt_bbox,dt_bbox)\n",
    "\n",
    "                    xA = int(round(min(gt_bbox[0], dt_bbox[0])))\n",
    "                    yA = int(round(min(gt_bbox[1], dt_bbox[1])))\n",
    "                    xB = int(round(max(gt_bbox[2], dt_bbox[2])))\n",
    "                    yB = int(round(max(gt_bbox[3], dt_bbox[3])))\n",
    "                    if gt_segm_decoded.shape != mask_util.decode(dt_segm[dt][1]).shape:\n",
    "                        print(yA,yB,xA,xB)\n",
    "                        print(gt_segm_decoded.shape,mask_util.decode(dt_segm[dt][1]).shape)\n",
    "                    \n",
    "                    cur_iou = IoU_Mask(gt_segm_decoded[yA:yB,xA:xB],\n",
    "                                       mask_util.decode(dt_segm[dt][1])[yA:yB,xA:xB])\n",
    "                    deviations.append(cur_iou)\n",
    "                else:\n",
    "                    deviations.append(0.0)\n",
    "            pos = max(range(len(deviations)), key=deviations.__getitem__)\n",
    "            if deviations[pos] >= threshold:\n",
    "                #Guarantee this instance isn't used again\n",
    "                used[pos] = True\n",
    "                return_group = []\n",
    "                for x in dt_segm[pos][2]:\n",
    "                    if x is None:\n",
    "                        return_group.append(np.zeros(img_size,dtype=np.uint8))\n",
    "                    else:\n",
    "                        return_group.append(mask_util.decode(x[1]))\n",
    "                pred_stack = np.dstack(return_group)\n",
    "                if pred_stack.shape[2] != 5:\n",
    "                    print(pred_stack.shape)\n",
    "                matched.append((pred_stack,mask_util.decode(gt_segm[gt]['segmentation'])))\n",
    "    \"\"\"\n",
    "    for i in range(0,len(used)):\n",
    "        if not used[i]:\n",
    "            dt_segm_decoded = mask_util.decode(dt_segm[dt][1])\n",
    "            dt_bbox = dt_segm[dt][0].copy()\n",
    "            dt_bbox[2] = dt_bbox[0]+dt_bbox[2]\n",
    "            dt_bbox[3] = dt_bbox[1]+dt_bbox[3]\n",
    "            xA,xB,yA,yB = int(round(dt_bbox[0])),int(round(dt_bbox[2])),int(round(dt_bbox[1])),int(round(dt_bbox[3]))\n",
    "            deviations = []\n",
    "            for c in crowds:\n",
    "                intersection = np.logical_and(mask_util.decode(gt_segm[c]['segmentation']),\n",
    "                                              dt_segm_decoded)\n",
    "                cur_iou = IoU_Mask(dt_segm_decoded[yA:yB,xA:xB],\n",
    "                                   intersection[yA:yB,xA:xB])\n",
    "                \n",
    "                fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(15,15))\n",
    "                ax=ax.flat\n",
    "                ax[0].set_title(\"Original Image\")  # set title\n",
    "                ax[0].imshow(image)\n",
    "                ax[1].set_title(\"DT SEGM\")  # set title\n",
    "                ax[1].imshow(dt_segm_decoded,cmap='gray',vmin=0,vmax=1)\n",
    "                ax[2].set_title(\"Intersection\")  # set title\n",
    "                ax[2].imshow(intersection,cmap='gray',vmin=0,vmax=1)\n",
    "                ax[3].set_title(\"Crowd\")  # set title\n",
    "                ax[3].imshow(mask_util.decode(gt_segm[c]['segmentation']),cmap='gray',vmin=0,vmax=1)\n",
    "                fig.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                deviations.append(cur_iou)\n",
    "            if len(crowds) != 0:\n",
    "                pos = max(range(len(deviations)), key=deviations.__getitem__)\n",
    "                if deviations[pos] < threshold:\n",
    "                    return_group = []\n",
    "                    for x in dt_segm[i][2]:\n",
    "                        if x is None:\n",
    "                            return_group.append(np.zeros(img_size,dtype=np.uint8))\n",
    "                        else:\n",
    "                            return_group.append(mask_util.decode(x[1]))\n",
    "                    pred_stack = np.dstack(return_group)    \n",
    "                    dt_unmatched.append((pred_stack,np.zeros(img_size,dtype=np.uint8)))\n",
    "            else:\n",
    "                return_group = []\n",
    "                for x in dt_segm[i][2]:\n",
    "                    if x is None:\n",
    "                        return_group.append(np.zeros(img_size,dtype=np.uint8))\n",
    "                    else:\n",
    "                        return_group.append(mask_util.decode(x[1]))\n",
    "                pred_stack = np.dstack(return_group)    \n",
    "                dt_unmatched.append((pred_stack,np.zeros(img_size,dtype=np.uint8)))\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return matched, dt_unmatched\n",
    "\n",
    "def matcher_no_img(dt,dataset,img=False):\n",
    "    result_dataset = []\n",
    "    gt = dataset.gt_return()\n",
    "    #print(gt.keys())\n",
    "    for i in tqdm(range(0,len(dataset))):        \n",
    "        filename = dataset[i]['img_metas'][0].data['filename']\n",
    "        size = dataset[i]['img_metas'][0].data['ori_shape']\n",
    "        \n",
    "        imgId = filename.split('/')[-1].split('.')[0]\n",
    "        cur_dt = dt[i]\n",
    "        cur_gt = gt[int(imgId)]\n",
    "        \n",
    "        #image = Image.open(filename)\n",
    "        #img_array = np.asarray(image)\n",
    "        \n",
    "        matched, unmatched = match_instances(cur_gt,cur_dt,(size[0],size[1]),DEVIATION_THRESHOLD_EVAL)\n",
    "        for m in matched:\n",
    "            res = m[0].copy()\n",
    "            result_dataset.append((filename,True,res,m[1]))\n",
    "        \"\"\"\n",
    "        for u in unmatched:\n",
    "            res = u[0].copy()\n",
    "            result_dataset.append((filename,False,res,u[1]))\n",
    "        \"\"\"\n",
    "            \n",
    "    return result_dataset\n",
    "\n",
    "def matcher(dt,dataset):\n",
    "    result_dataset = []\n",
    "    gt = dataset.gt_return()\n",
    "    for i in tqdm(range(0,len(dataset))):\n",
    "        filename = dataset[i]['img_metas'][0].data['filename']\n",
    "        size = dataset[i]['img_metas'][0].data['ori_shape']\n",
    "        \n",
    "        imgId = filename.split('/')[-1].split('.')[0]\n",
    "        cur_dt = dt[i]\n",
    "        cur_gt = gt[int(imgId)]\n",
    "        image = Image.open(filename)\n",
    "        img_array = np.asarray(image)\n",
    "        \n",
    "        matched, unmatched = match_instances(cur_gt,cur_dt,(size[0],size[1]),DEVIATION_THRESHOLD_EVAL)\n",
    "        for m in matched:\n",
    "            res = np.dstack((img_array,m[0]))\n",
    "            result_dataset.append((filename,True,res,m[1]))\n",
    "        \"\"\"for u in unmatched:\n",
    "            res = np.dstack((img_array,u[0]))\n",
    "            result_dataset.append((filename,res,u[1]))\"\"\"\n",
    "            \n",
    "    return result_dataset\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0da963-e344-4faf-9c08-8d18f3e52ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "dataset, data_loader = get_dataset()\n",
    "results = get_results(model_dict, CREDIBILITY_THRESHOLD,True,ENSEMBLE_METHOD,dataset,result_type='segm')\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b46bcf-89ae-4d87-a866-be6093317a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b8340-e722-4109-9b5e-2b0c041b8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, data_loader = get_dataset()\n",
    "final_results = matcher(results,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c6e98-4781-40d3-9a5b-7b803a9a445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c74ce-fdf9-4678-8387-8f7a5c29e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count = 0\n",
    "\n",
    "for (filename,_,dt_segm,gt_segm) in tqdm(final_results):\n",
    "    if dt_segm.shape[2] != 8:\n",
    "        img = cv2.cvtColor(dt_segm[:,:,0],cv2.COLOR_GRAY2RGB)\n",
    "        dt = dt_segm[:,:,1:]\n",
    "    else:\n",
    "        img = dt_segm[:,:,0:3]\n",
    "        dt = dt_segm[:,:,3:]\n",
    "    gt = gt_segm\n",
    "    \n",
    "    cv2.imwrite(\"data/coco_bitwise_or_reduced_ensemble_results/img/\"+str(count)+\".png\",img)\n",
    "    cv2.imwrite(\"data/coco_bitwise_or_reduced_ensemble_results/net1/\"+str(count)+\".png\",dt[:,:,0]*255)\n",
    "    cv2.imwrite(\"data/coco_bitwise_or_reduced_ensemble_results/net2/\"+str(count)+\".png\",dt[:,:,1]*255)\n",
    "    cv2.imwrite(\"data/coco_bitwise_or_reduced_ensemble_results/net3/\"+str(count)+\".png\",dt[:,:,2]*255)\n",
    "    cv2.imwrite(\"data/coco_bitwise_or_reduced_ensemble_results/net4/\"+str(count)+\".png\",dt[:,:,3]*255)\n",
    "    cv2.imwrite(\"data/coco_bitwise_or_reduced_ensemble_results/net5/\"+str(count)+\".png\",dt[:,:,4]*255)\n",
    "    cv2.imwrite(\"data/coco_bitwise_or_reduced_ensemble_results/gt/\"+str(count)+\".png\",gt*255)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196dca39-90e3-444c-9251-ea47229e569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('work_dirs/dataset_generation/reduced_coco_train_average_img.pkl', 'wb') as handle:\n",
    "    #compressed_file = bz2.BZ2File(handle, 'w')\n",
    "    pickle.dump(final_results,handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e2e88-0422-4f6c-9c01-77cdfd2cc94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('work_dirs/dataset_generation/coco_average_no_img.pkl', 'rb') as handle:\n",
    "    #compressed_file = bz2.BZ2File(handle, 'w')\n",
    "    results = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9fe0df-709b-4e8a-93a7-386c642d8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f08381-9101-4286-ae9e-217832dbb77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in range(0,len(final_results)):\n",
    "    if not final_results[img][1]:\n",
    "        print(final_results[img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c0512-358d-4b61-b413-1eda4c3b0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "from multiprocessing import Pool\n",
    "from enum import Enum\n",
    "\n",
    "class Direction(Enum):\n",
    "    N = 0\n",
    "    NE = 1\n",
    "    E = 2\n",
    "    SE = 3\n",
    "    S = 4\n",
    "    SW = 5\n",
    "    W = 6\n",
    "    NW = 7\n",
    "\n",
    "transform = {\n",
    "    'N': (0,-1),\n",
    "    'NE': (1,-1),\n",
    "    'E': (1,0),\n",
    "    'SE': (1,1),\n",
    "    'S': (0,1),\n",
    "    'SW': (-1,1),\n",
    "    'W': (-1,0),\n",
    "    'NW': (-1,-1)\n",
    "}\n",
    "\n",
    "\"\"\"def _is_border(kernel):\n",
    "    if kernel[0][1] and kernel[1][0] and kernel[2][1] and kernel[1][2]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\"\"\"\n",
    "\n",
    "# [[0,1,0],\n",
    "#  [1,1,1],\n",
    "#  [0,1,0]]\n",
    "\n",
    "def _form_boundary_mask(mask):\n",
    "    padded_mask = np.pad(mask, [(32,32),(32,32)], mode='constant', constant_values=[(0,0),(0,0)])\n",
    "    padded_mask_shape = padded_mask.shape\n",
    "    #boundary_mask = np.zeros(mask.shape,dtype=np.bool_)\n",
    "    kernel = [[0,1,0],\n",
    "              [1,5,1],\n",
    "              [0,1,0]]\n",
    "    boundary_mask = scipy.signal.convolve2d(padded_mask,kernel,boundary=\"fill\",fillvalue=0.0)\n",
    "    boundary_mask = (boundary_mask < 9.0) & (boundary_mask > 5.0)\n",
    "    \n",
    "    return boundary_mask\n",
    "\n",
    "def _transform_dir(direction):\n",
    "    return transform[Direction(direction).name]\n",
    "\n",
    "def _find_next_dir(boundary_mask,cur_pixel,last_dir):\n",
    "    next_dir = Direction((Direction[last_dir].value-3)%8).name\n",
    "    inv_dir = Direction((Direction[last_dir].value-4)%8).name\n",
    "    \n",
    "    while True:\n",
    "        next_pixel = [sum(x) for x in zip(cur_pixel,transform[next_dir])]\n",
    "        if next_dir == inv_dir:\n",
    "            return None\n",
    "        elif boundary_mask[next_pixel[1]][next_pixel[0]] == 1:\n",
    "            return next_pixel,next_dir\n",
    "        else:\n",
    "            next_dir = Direction((Direction[next_dir].value+1)%8).name\n",
    "    \n",
    "        \n",
    "\n",
    "def _find_highest_pixel(boundary):\n",
    "    if len(np.where(boundary == 1)[0]) > 0:\n",
    "        y = min(np.where(boundary == 1)[0])\n",
    "        x = min(np.where(boundary[y] == 1)[0])\n",
    "        return (x,y)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def _order_boundary_pixels(boundary):    \n",
    "    ordered_pixels = []\n",
    "    \n",
    "    exists_bounds = True\n",
    "\n",
    "    cur_pixel = _find_highest_pixel(boundary)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(boundary,cmap='gray')\n",
    "    plt.show()\n",
    "    cv2.imwrite(\"../boundary.png\",boundary*255)\n",
    "    \n",
    "    while exists_bounds:\n",
    "        cur_list = []\n",
    "        \n",
    "        cur_list.append(cur_pixel)\n",
    "        last_dir = \"E\"\n",
    "        exists_next = True \n",
    "\n",
    "        while exists_next:\n",
    "            boundary[cur_pixel[1],cur_pixel[0]] = 0.0\n",
    "\n",
    "            res = _find_next_dir(boundary,cur_pixel,last_dir)\n",
    "            if not res:\n",
    "                exists_next = False\n",
    "            else:\n",
    "                cur_pixel,last_dir = res\n",
    "                cur_list.append(cur_pixel)        \n",
    "        \n",
    "        ordered_pixels.append(cur_list)\n",
    "        cur_pixel = _find_highest_pixel(boundary)\n",
    "        if not cur_pixel:\n",
    "            exists_bounds = False\n",
    "    \n",
    "    return ordered_pixels\n",
    "\n",
    "def gen_sliding_window(mask):\n",
    "    boundary_mask = _form_boundary_mask(mask)\n",
    "    temp_mask = boundary_mask.copy()\n",
    "    padded_mask = np.pad(mask.copy(), [(32,32),(32,32)], mode='constant', constant_values=[(0,0),(0,0)])\n",
    "    print(padded_mask.shape)\n",
    "    sliding_window = _order_boundary_pixels(temp_mask)\n",
    "    \n",
    "    for group in sliding_window:\n",
    "        count = 0\n",
    "        \n",
    "        for pixel in group[::32]:\n",
    "            print(pixel)\n",
    "            square = padded_mask[pixel[1]-31:pixel[1]+32,pixel[0]-31:pixel[0]+32]\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.imshow(square,cmap='gray')\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "    return sliding_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fbb8c-53de-4765-b178-7e49b27b6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for (n,(filename,_,dt_segm,gt_segm)) in enumerate(final_results,0):\n",
    "    print(dt_segm.shape)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(dt_segm[:,:,0:3])\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"img = cv2.imread(\"../img.png\",cv2.IMREAD_GRAYSCALE)\n",
    "#img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "print(img.shape)\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.show()\n",
    "windows = gen_sliding_window(img/255)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b764471-8a94-4a7c-808e-b47c357c88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import bz2\n",
    "\n",
    "with open('work_dirs/dataset_generation/cityscapes_full.pkl', 'wb') as handle:\n",
    "    #compressed_file = bz2.BZ2File(handle, 'w')\n",
    "    pickle.dump(final_results, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d1f5c-02ba-4e76-89f3-8579008deebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec0dc4-a052-49d2-be7a-b52f0b720ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38eb86-af25-45f8-bec9-edc80f7c12f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e41d8-1db3-4944-ba6b-5f68c725db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"Loading json...\")\n",
    "\n",
    "f = open('data/coco/annotations/instances_val2017.json')\n",
    "\n",
    "data = json.load(f)\n",
    "\n",
    "print(\"Json Correctly Loaded\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617dabf4-948d-425b-be56-252829ac8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a35cd-1eb8-4921-b0bb-ed46222cea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['images'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d6371-059d-41a7-9c02-81948de683a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_arr = []\n",
    "has_crowd = []\n",
    "\n",
    "#for im in data[\"images\"]:\n",
    "    #id_arr.append(im['id'])\n",
    "#has_crowd = list.fromkeys(id_arr)\n",
    "\n",
    "#isolate annotations only referring to people\n",
    "only_people_ann = [x for x in data[\"annotations\"] if x[\"category_id\"] == 1]\n",
    "\n",
    "#mark crowded images\n",
    "images_with_crowds = [x[\"image_id\"] for x in only_people_ann if x[\"iscrowd\"] == 1]\n",
    "\n",
    "#remove images that have crowds\n",
    "final_images = [x for x in data[\"images\"] if not x[\"id\"] in images_with_crowds]\n",
    "\n",
    "#remove annotations referring to images that have crowds\n",
    "final_annotations = [x for x in only_people_ann if not x[\"image_id\"] in images_with_crowds]\n",
    "\n",
    "#remove every category besides person\n",
    "final_categories = [data[\"categories\"][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02fa329-c002-43dc-89a0-44b72dfac294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d11398-b019-4964-8abb-7f5f60190e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421092ae-96f2-4956-922d-b1a89f8c1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299947cf-ce19-4007-b094-25ac2b470d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe8180-8495-4329-a7ee-eb47fe501430",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"info\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bec48d-820b-4b67-bb87-1669d697c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'info': data['info'],\n",
    "        'licenses': data['licenses'],\n",
    "        'images': final_images,\n",
    "        'annotations': final_annotations,\n",
    "        'categories': final_categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de002e1c-758a-4719-a960-42f2c8f4ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/coco/annotations/instances_val2017_person_no_crowds.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2cc77-9b32-44a8-b387-3d97cdc1062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_images = data['images'][0:3000]\n",
    "print(len(new_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc19c75-5435-4d39-bc96-d3446189dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_arr = []\n",
    "for im in new_images:\n",
    "    id_arr.append(im['id'])\n",
    "id_dict = dict.fromkeys(id_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72fb95-a8ae-4ec9-8352-22fc75ac7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotations = []\n",
    "for ann in data['annotations']:\n",
    "    if ann['image_id'] in id_dict:\n",
    "        new_annotations.append(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66370ae-5256-4dca-a126-381e201a3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_annotations))\n",
    "print(len(data['annotations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d62b4-0ae2-4be3-9483-24065dac73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2621f13-fd47-4625-b9eb-738f27e4d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['annotations'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cda072-be29-4e4e-8292-391ac9953e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
